# ---- SAMPLE YAML FILE TO CONFIGURE METRIC RUNNER ---- #

# !MANDATORY! Metrics and Metrics directories be executed
metrics:
  - /path/to/metric-1
  - /path/to/metric-2

# Parquet or JSON Tables files paths
inputs:
 input_1: parquet/input_1.parquet
 input_2: parquet/input_2.parquet
 input_3: parquet/input_3.parquet

# dateRange section allows defining dynamic date range for multiple folder names
# in case a dynamic date parameter was defined in 'inputs' section as '%s' , for example- userAggregatedData: /path/to/data/%s/
dateRange:
  input_1: yyyy/MM/dd:2017/09/01:2017/09/20
  input_2: yyyy/MM/dd:2017/09/01:2017/09/20

# Set custom variables that would be accessible from the SQL
variables:
 StartDate: 2017/09/01
 EndDate: 2017/09/20
 TrimmedDateFormat: yyyy/MM/dd

outputs:
  # cassandra Database arguments: host, username and password are supported
  cassandra:
    connection.host: example.cassandra.db
  # Redshift Database arguments: host, username, password, s3_access_key and s3_secret are supported
  redshift:
    jdbcURL: jdbc:redshift://<IP>:<PORT>/file?user=username&password=pass
    tempS3Dir: s3://path/to/redshift/temp/dir/
  # Redis Database arguments: host, port and db are supported
  redis:
    host: hostname
    port: port-number
    auth: authentication
    db: database
  # Segment API Key
  segment:
    apiKey: apikey
  # Output file for csv or parquet
  file: /path/to/parquet/output

# If set to true, triggers Explain before saving
explain: true

# Shows a Preview of the output
showPreviewLines: 42

# Set Log Level : ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN
logLevel: WARN

# Set Application Name to have app name prefix in spark instrumentation counters
appName: appName