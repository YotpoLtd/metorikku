# ---- SAMPLE YAML OF JOB CONFIG FILE ---- #

# !MANDATORY! Metrics and Metrics directories be executed
metrics:
  - /path/to/metric-1
  - /path/to/metric-2

# Input configuration
inputs:
  input_1:
    file:
      path: parquet/input_1.parquet
  input_2:
    file:
      path: parquet/input_2.parquet
  input_3:
      file_date_range:
        template: parquet/%s/input_1.parquet
        date_range:
         format: yyyy/MM/dd
         startDate: 2017/09/01
         endDate: 2017/09/03
  input_4:
    jdbc:
      connectionUrl: jdbc:mysql://localhost/db?zeroDateTimeBehavior=convertToNull
      user: user
      password: pass
      table: some_table
      # You can optionally add here any supported option from https://spark.apache.org/docs/latest/sql-programming-guide.html#jdbc-to-other-databases
      options:
        numPartitions: 100
        driver: com.mysql.jdbc.Driver
  input_5:
      kafka:
        servers:
          - localhost:9092
        topic: some_topic
  input_6:
    cassandra:
      host: 127.0.0.1
      user: user
      password: password
      table: table
      keySpace: keySpace
      options:
        ....

# Set custom variables that would be accessible from the SQL
variables:
 StartDate: 2017/09/01
 EndDate: 2017/09/20
 TrimmedDateFormat: yyyy/MM/dd

output:
  # cassandra Database arguments: host is mandatory. username and password are supported
  cassandra:
    host: example.cassandra.db
    username: user
    password: password
  # Redshift Database arguments: jdbcURL and tempS3Dir are mandatory.
  redshift:
    jdbcURL: jdbc:redshift://<IP>:<PORT>/file?user=username&password=pass
    tempS3Dir: s3://path/to/redshift/temp/dir/
  # Redis Database arguments: host is mandatory. port, auth and db are supported
  redis:
    host: hostname
    port: port-number
    auth: authentication
    db: database
  # Segment API Key
  segment:
    apiKey: apikey
  # Output file directory
  file:
    dir: /path/to/parquet/output
  # JDBC database
  jdbc:
    connectionUrl: "jdbc:postgresql://localhost:5432/databasename"
    user: username
    password: password
    driver: "org.postgresql.Driver"

# If set to true, triggers Explain before saving
explain: true

# Shows a Preview of the output
showPreviewLines: 42

# Set Log Level : ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN
logLevel: WARN

# Set Application Name to have app name prefix in spark instrumentation counters
appName: appName

# Set instrumentation writer (default is spark metrics)
instrumentation:
  influxdb:
    url: http://localhost:8086
    username: username
    password: password
    dbName: test